{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a25aaf-fff2-4004-9ffe-6bee3356e492",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# EML4 - Aula 05 - Processamento de Massivo de Dados\n",
    "### Preparação de dados e Aprendizado de Máquina em Spark\n",
    "\n",
    "\n",
    "Principais links:\n",
    "* [Spark docs](https://spark.apache.org/docs/latest/)\n",
    "* [Datasets Compactados](https://drive.google.com/file/d/1MdZGO9quJVxuq5wcBafT25kA2UR4C1uG/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2a38f5-4817-4724-9d91-ea16e45eb207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Importações e inicialização da sessão Spark para Notebooks fora do Databricks\n",
    "\n",
    "#import findspark, pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "#findspark.init()\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Checando se sessão Spark está funcionando\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02bc543-dc4d-4ff6-adec-9ae83bd86912",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preparação\n",
    "\n",
    "Carregando o data set. O arquivo [crimes.csv](https://www.kaggle.com/ankkur13/boston-crime-data) deve ser adicionado ao cluster e caminho colocado na célula abaixo.\n",
    "\n",
    "Este data set possui formato csv (comma-separated values) e possui o título das colunas, mas Spark aceita diferentes valores como Parquet, ORC, JDBC, LIBSVM e outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dadf23e-bd7a-459d-92e1-897197ca65d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_location = 'dbfs:/FileStore/shared_uploads/naldi@ufscar.br/crime.csv'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', True) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca5f140-7a7a-4254-9f7e-f5a605d10f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Aternativamente dataset.show(?)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d14aca8-52f8-493e-bf32-4f4f6b9739a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08109718-0ac6-4312-9601-30e720e71bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Transformers e Estimators\n",
    "\n",
    "**Exemplo 1**: Tokenizer é transformer para manipulação de dados. Seu objetivo é quebrar uma String em um vetor de tokens que podem ser manipulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f66a96-0b07-44c4-b2dd-8cbbca46bd80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol='OFFENSE_CODE_GROUP', outputCol='OFFENSE_CODE_TOKENIZED')\n",
    "df = dataset.select('OFFENSE_CODE_GROUP').distinct()\n",
    "tkn.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaf6464-f8b7-47e5-ba30-c6d05e307775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exemplo 2**: StandardScaler é um estimator com o objetivo de ajustar valores contínuos de forma obter um valor médio = 0 e/ou um desvio padrão = 1\n",
    "\n",
    "Vamos padronizar os dados da localidade dos dados, filtrando dados nulos e inconsistentes? Veja o exemplo com a latitude\n",
    "\n",
    "StandardScaler deve receber um vetor numerico como entrada, por isso utilizamos a transformação VectorAssembler para organizar os dados em um vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7d1f8c-a995-4be5-914a-3ad6a4550f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "df = dataset.select('Lat').filter('Lat is not null').filter('Lat > 0')\n",
    "local_assembler = VectorAssembler(inputCols=[\"Lat\"], outputCol=\"VetLat\")\n",
    "dfv = local_assembler.transform(df)\n",
    "mms = StandardScaler(inputCol='VetLat', outputCol='LatStd', withStd=True, withMean=True)\n",
    "mms.fit(dfv).transform(dfv).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d27937-4c09-4979-888c-39f4a2aa27c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 1\n",
    "Outro estimator, semelhante ao StandardScaler é o MinMaxScaler, seu objetivo é transformar valores de forma a se ajustarem a um limite mínimo e máximo. Seu uso mais comum é na normalização de valores para o intervalo [0,1]\n",
    "\n",
    "Estude as características do atributo Longitude, selecione latutide e longitude válidos e aplique o estimator MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0921eaf0-ba70-43e8-b6f1-9bdf53e9b56b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#faça aqui o exercício 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4536a1-b217-4f29-b960-e005e608fd63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imputer\n",
    "\n",
    "**Exemplo 3**: Tratando valores ausentes com Imputer, substituindo pela média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92bd9a2-8899-4976-a025-3cff5730a3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = dataset.select(\"Lat\",\"Long\")\n",
    "imput = Imputer(inputCols=[\"Lat\",\"Long\"], outputCols = [\"NeWLat\",\"NeWLong\"])\n",
    "modelo = imput.fit(df)\n",
    "df = modelo.transform(df)\n",
    "df.filter('Lat is null').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03084c66-d611-4891-a468-6c81a19c4cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RFormula\n",
    "\n",
    "**Exemplo 4**: Utilizando o estimator RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9be9c15-b653-4d6f-be5c-e0f2284c20e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "df = dataset\\\n",
    "  .select('OFFENSE_CODE_GROUP', 'YEAR', 'MONTH', 'HOUR', 'DISTRICT')\\\n",
    "  .where('DISTRICT is not null')\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ . -DISTRICT')\n",
    "rf.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb818b4d-ef8c-4c57-9bd5-d7978a40b80d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exemplo 4**: Estruturando atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af37dea-487e-4d0e-b7b3-e2bac06585cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = dataset\\\n",
    "  .select('OFFENSE_CODE_GROUP', 'DAY_OF_WEEK', 'HOUR')\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ DAY_OF_WEEK + HOUR')\n",
    "rf.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78b5067-1150-4f00-baaa-82f2e721aa9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 2\n",
    "\n",
    "Utilizando RFormula, prepare um DataFrame para uma tarefa de classificação, onde o label será `OFFENSE_CODE_GROUP`, e os atributos serão: `DISTRICT`, `DAY_OF_WEEK` e `HOUR`.\n",
    "\n",
    "Será necessário tratar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce0559c-3fa5-417b-be58-b011279ec5ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#faça aqui o exercício 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ace75c-13e0-4694-ad7a-61ea69a16bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Classificação\n",
    "\n",
    "Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb423c82-a5cd-43ce-8eab-1ad3368d27d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_location = '/FileStore/tables/covtype.data'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location) \\\n",
    "               .withColumnRenamed('_c54', 'class')\n",
    "\n",
    "rf = RFormula(formula = 'class ~ .')\n",
    "bInput = rf.fit(dataset) \\\n",
    "           .transform(dataset) \\\n",
    "           .select('features', 'label')\n",
    "\n",
    "bInput.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc3872-7c36-4808-b327-7e5d4629297a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d4e315-df15-4a24-aab7-83331dadb117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e5c937-c3ad-4ad0-a429-f37e0955d96a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad90ea9a-667a-4f1a-afd8-9a238ce56b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"accuracy: \", model.summary.accuracy)\n",
    "print(\"precision by label: \", model.summary.precisionByLabel)\n",
    "print(\"recall by label: \", model.summary.recallByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee549fb-8b96-45d1-8dc1-b3fe1e446a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test = bInput.sample(fraction=0.0001)\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94598e3f-7c8d-4884-839c-1d221aeff578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32369a6c-4149-4c0b-82d5-193a8828f0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "train, test = bInput.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "model = dt.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f6afa2-8250-440f-b695-71a8784b3602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1501955-0dad-4c44-beb1-cddcea2b1445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 3\n",
    "\n",
    "* Carregue os dados do dataset `Crimes in Boston`\n",
    "* Pré-processe os dados para uma tarefa de classificação. A coluna `OFFENSE_CODE_GROUP` será usada como classe e as colunas `DISTRICT`, `DAY_OF_WEEK`, `HOUR` e `SHOOTING` serão usadas como atributos.\n",
    " * Registros sem informações de `DISTRICT` não devem entrar na análise\n",
    " * Registros sem informação de `SHOOTING` devem assumir valor `N` (dica: usar `expr` e `coalesce`)\n",
    "* Crie um modelo de Regressão Logística\n",
    "* Avalie o modelo pela métrica accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acec865-afc5-421c-84c4-754f408e8d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#faça aqui o exercício 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c756ab-f822-4f64-9057-2cda87047d90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Agrupamento\n",
    "\n",
    "Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21c47a6-74a1-4c5a-8a25-f65cbcb841f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_location = '/FileStore/tables/covtype.data'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location) \\\n",
    "               .withColumnRenamed('_c54', 'class')\n",
    "\n",
    "rf = RFormula(formula = ' ~ . -class')\n",
    "bInput = rf.fit(dataset) \\\n",
    "           .transform(dataset) \\\n",
    "           .select('features', 'class')\n",
    "\n",
    "bInput.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa8edb0-3a45-4ecc-813b-f4c770e93e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1228c5e4-8421-42b0-90a0-c2cbc8759744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(7)\n",
    "model = kmeans.fit(bInput)\n",
    "\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5faff66d-34b5-492b-b487-511b31b6383a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(bInput)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64be45c7-50fb-4c61-b92b-eb3f900dfe94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"silhouette: \", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052fe035-135e-455b-9c92-9c31ca744d50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 4\n",
    "\n",
    "* Carregue os dados do dataset `Crimes in Boston`\n",
    "* Pré-processe os dados para uma tarefa de agrupamento. As colunas `Lat` (latitude) e `Long` (longitude) serão usadas como atributos.\n",
    " * Remover ou imputar valores nulos ou inconsistentes para ambos os atributos\n",
    "* Crie um modelo de agrupamento pelo algoritmo k-means\n",
    "* Avalie o modelo pelo índice silhueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149825f8-fb8f-46b1-8957-1c9363c041a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#faça aqui o exercício 04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1b48f7-5d11-48e6-94c1-bda15c8e5187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline\n",
    "\n",
    "É possível executar todos os objetos (modelos e transformações) instanciados em um *pipeline* de aprendizado de máquina. Isso facilita a aplicação de modelos e reutilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e284ff-3ec3-42b1-839d-dcfc839b13bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "iris = spark.read.csv(\"dbfs:/FileStore/shared_uploads/naldi@ufscar.br/iris.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "irisTreino, irisTeste = iris.randomSplit([0.7,0.3])\n",
    "\n",
    "vector = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],outputCol=\"features\" )\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"target\")\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[4,5,3], featuresCol=\"features\", labelCol=\"target\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector, indexer, mlp])\n",
    "modelo = pipeline.fit(irisTreino)\n",
    "previsao = modelo.transform(irisTeste)\n",
    "\n",
    "previsao.select(\"target\",\"features\",\"rawprediction\",\"probability\",\"prediction\").show(5, truncate=False)\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\",predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = performance.evaluate(previsao)\n",
    "print(acuracia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04bb6e75-b369-42b2-8650-a001795fd522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Crossvalidation e Tunning\n",
    "\n",
    "Ao desenvolver aplicações com aprendizado de máquina, é possível buscar por parametrização adequada aos dados. É possível buscar pela melhor configuração utilizando *grid search* em conjunto com um bom estimador como validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597d9edd-ebf6-4616-8a75-eec21c483349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "iris = spark.read.csv(\"dbfs:/FileStore/shared_uploads/naldi@ufscar.br/iris.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "irisTreino, irisTeste = iris.randomSplit([0.8,0.2])\n",
    "\n",
    "vector = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],outputCol=\"features\" )\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"target\")\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[4,5,3], featuresCol=\"features\", labelCol=\"target\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector, indexer, mlp])\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\", metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(mlp.maxIter,[10,100,1000]).addGrid(mlp.layers,[[4,4,4,3],[4,6,3]]).build()\n",
    "\n",
    "#A validação cruzada tem seu próprio avaliador e vai subdividir os dados de treino para o processo de tunning\n",
    "crossval = CrossValidator(estimator=pipeline,estimatorParamMaps=grid,evaluator=performance,numFolds=5)\n",
    "\n",
    "#Podemos utilizar e avaliar novamente o modelos escolhido pela validação cruzada (validação da validação) usando hold out\n",
    "modelo = crossval.fit(irisTreino)\n",
    "previsao = modelo.transform(irisTeste)\n",
    "previsao.select(\"rawprediction\",\"probability\",\"prediction\").show(5, truncate=False)\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = performance.evaluate(previsao)\n",
    "print(acuracia)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47fe01c6-67ea-4f4c-993c-ad275aac0c56",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 7\n",
    "\n",
    "Escolha outro conjunto de dados e induza um modelo preditivo de classificação/regressão utilizando validação cruzada e ajuste de parâmetros. Avalie o resultado escolhido por meio de acurácia. \n",
    "\n",
    "Utilize sua documentação disponível em [aqui!](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html?highlight=crossvalidator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362be3f1-bea2-4273-9e25-b46dac153076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#faça aqui o exercício 07"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "EML4 Aula 05 - Aluno",
   "widgets": {}
  },
  "name": "Aula 7",
  "notebookId": 4064479383097879
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
