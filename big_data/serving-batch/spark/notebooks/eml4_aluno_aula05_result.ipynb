{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a25aaf-fff2-4004-9ffe-6bee3356e492",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# EML4 - Aula 05 - Processamento de Massivo de Dados\n",
    "### Preparação de dados e Aprendizado de Máquina em Spark\n",
    "\n",
    "\n",
    "Principais links:\n",
    "* [Spark docs](https://spark.apache.org/docs/latest/)\n",
    "* [Datasets Compactados](https://drive.google.com/file/d/1MdZGO9quJVxuq5wcBafT25kA2UR4C1uG/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2a38f5-4817-4724-9d91-ea16e45eb207",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://782a6cfc0c79:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a553174e6b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importações e inicialização da sessão Spark para Notebooks fora do Databricks\n",
    "\n",
    "#import findspark, pyspark\n",
    "#from pyspark.sql import SparkSession\n",
    "#findspark.init()\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Checando se sessão Spark está funcionando\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02bc543-dc4d-4ff6-adec-9ae83bd86912",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Preparação\n",
    "\n",
    "Carregando o data set. O arquivo [crimes.csv](https://www.kaggle.com/ankkur13/boston-crime-data) deve ser adicionado ao cluster e caminho colocado na célula abaixo.\n",
    "\n",
    "Este data set possui formato csv (comma-separated values) e possui o título das colunas, mas Spark aceita diferentes valores como Parquet, ORC, JDBC, LIBSVM e outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dadf23e-bd7a-459d-92e1-897197ca65d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset_location = 'crime.csv'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', True) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca5f140-7a7a-4254-9f7e-f5a605d10f5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[INCIDENT_NUMBER: string, OFFENSE_CODE: int, OFFENSE_CODE_GROUP: string, OFFENSE_DESCRIPTION: string, DISTRICT: string, REPORTING_AREA: string, SHOOTING: string, OCCURRED_ON_DATE: timestamp, YEAR: int, MONTH: int, DAY_OF_WEEK: string, HOUR: int, UCR_PART: string, STREET: string, Lat: double, Long: double, Location: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Aternativamente dataset.show(?)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d14aca8-52f8-493e-bf32-4f4f6b9739a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- INCIDENT_NUMBER: string (nullable = true)\n",
      " |-- OFFENSE_CODE: integer (nullable = true)\n",
      " |-- OFFENSE_CODE_GROUP: string (nullable = true)\n",
      " |-- OFFENSE_DESCRIPTION: string (nullable = true)\n",
      " |-- DISTRICT: string (nullable = true)\n",
      " |-- REPORTING_AREA: string (nullable = true)\n",
      " |-- SHOOTING: string (nullable = true)\n",
      " |-- OCCURRED_ON_DATE: timestamp (nullable = true)\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: string (nullable = true)\n",
      " |-- HOUR: integer (nullable = true)\n",
      " |-- UCR_PART: string (nullable = true)\n",
      " |-- STREET: string (nullable = true)\n",
      " |-- Lat: double (nullable = true)\n",
      " |-- Long: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08109718-0ac6-4312-9601-30e720e71bd0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Transformers e Estimators\n",
    "\n",
    "**Exemplo 1**: Tokenizer é transformer para manipulação de dados. Seu objetivo é quebrar uma String em um vetor de tokens que podem ser manipulados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1f66a96-0b07-44c4-b2dd-8cbbca46bd80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===>                                                     (1 + 14) / 15]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+\n",
      "|  OFFENSE_CODE_GROUP|OFFENSE_CODE_TOKENIZED|\n",
      "+--------------------+----------------------+\n",
      "|             Larceny|             [larceny]|\n",
      "| Auto Theft Recovery|  [auto, theft, rec...|\n",
      "|   Firearm Discovery|  [firearm, discovery]|\n",
      "|Recovered Stolen ...|  [recovered, stole...|\n",
      "|License Plate Rel...|  [license, plate, ...|\n",
      "|   License Violation|  [license, violation]|\n",
      "|Motor Vehicle Acc...|  [motor, vehicle, ...|\n",
      "|    Liquor Violation|   [liquor, violation]|\n",
      "|Assembly or Gathe...|  [assembly, or, ga...|\n",
      "|      Property Found|     [property, found]|\n",
      "|      Simple Assault|     [simple, assault]|\n",
      "|     Warrant Arrests|    [warrant, arrests]|\n",
      "|Prisoner Related ...|  [prisoner, relate...|\n",
      "|      Drug Violation|     [drug, violation]|\n",
      "|             Robbery|             [robbery]|\n",
      "|        Embezzlement|        [embezzlement]|\n",
      "|Missing Person Lo...|  [missing, person,...|\n",
      "|Investigate Property|  [investigate, pro...|\n",
      "|  Firearm Violations|  [firearm, violati...|\n",
      "|               Towed|               [towed]|\n",
      "+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer(inputCol='OFFENSE_CODE_GROUP', outputCol='OFFENSE_CODE_TOKENIZED')\n",
    "df = dataset.select('OFFENSE_CODE_GROUP').distinct()\n",
    "tkn.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaaf6464-f8b7-47e5-ba30-c6d05e307775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exemplo 2**: StandardScaler é um estimator com o objetivo de ajustar valores contínuos de forma obter um valor médio = 0 e/ou um desvio padrão = 1\n",
    "\n",
    "Vamos padronizar os dados da localidade dos dados, filtrando dados nulos e inconsistentes? Veja o exemplo com a latitude\n",
    "\n",
    "StandardScaler deve receber um vetor numerico como entrada, por isso utilizamos a transformação VectorAssembler para organizar os dados em um vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7d1f8c-a995-4be5-914a-3ad6a4550f62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------------------+\n",
      "|Lat        |VetLat       |LatStd                |\n",
      "+-----------+-------------+----------------------+\n",
      "|42.26260773|[42.26260773]|[-1.8734208647489472] |\n",
      "|42.35211146|[42.35211146]|[0.935988368931435]   |\n",
      "|42.30812619|[42.30812619]|[-0.44465379742207745]|\n",
      "|42.35945371|[42.35945371]|[1.1664523389354866]  |\n",
      "|42.37525782|[42.37525782]|[1.6625234251808525]  |\n",
      "|42.29919694|[42.29919694]|[-0.7249316964531362] |\n",
      "|42.32073413|[42.32073413]|[-0.04890645757743463]|\n",
      "|42.33380683|[42.33380683]|[0.3614291126735248]  |\n",
      "|42.25614494|[42.25614494]|[-2.076279694222733]  |\n",
      "|42.348866  |[42.348866]  |[0.8341174715783722]  |\n",
      "|42.34432328|[42.34432328]|[0.6915272184790474]  |\n",
      "|42.32324363|[42.32324363]|[0.029863583312779347]|\n",
      "|42.26059891|[42.26059891]|[-1.9364751917228349] |\n",
      "|42.27986526|[42.27986526]|[-1.3317287572522039] |\n",
      "|42.27791927|[42.27791927]|[-1.3928109297455482] |\n",
      "|42.31596119|[42.31596119]|[-0.198723026202991]  |\n",
      "|42.28076737|[42.28076737]|[-1.3034126617800843] |\n",
      "|42.31277782|[42.31277782]|[-0.2986449967423941] |\n",
      "|42.34268073|[42.34268073]|[0.6399696449926617]  |\n",
      "|42.30998781|[42.30998781]|[-0.38621989285069347]|\n",
      "+-----------+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "df = dataset.select('Lat').filter('Lat is not null').filter('Lat > 0')\n",
    "local_assembler = VectorAssembler(inputCols=[\"Lat\"], outputCol=\"VetLat\")\n",
    "dfv = local_assembler.transform(df)\n",
    "mms = StandardScaler(inputCol='VetLat', outputCol='LatStd', withStd=True, withMean=True)\n",
    "mms.fit(dfv).transform(dfv).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d27937-4c09-4979-888c-39f4a2aa27c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 1\n",
    "Outro estimator, semelhante ao StandardScaler é o MinMaxScaler, seu objetivo é transformar valores de forma a se ajustarem a um limite mínimo e máximo. Seu uso mais comum é na normalização de valores para o intervalo [0,1]\n",
    "\n",
    "Estude as características do atributo Longitude, selecione latutide e longitude válidos e aplique o estimator MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0921eaf0-ba70-43e8-b6f1-9bdf53e9b56b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------------+-----------------------------------------+\n",
      "|Lat        |Long        |Local                     |LocalStd                                 |\n",
      "+-----------+------------+--------------------------+-----------------------------------------+\n",
      "|42.26260773|-71.12118637|[42.26260773,-71.12118637]|[0.1856653098710857,0.2673862497925831]  |\n",
      "|42.35211146|-71.13531147|[42.35211146,-71.13531147]|[0.7360230336323171,0.20168738604231215] |\n",
      "|42.30812619|-71.07692974|[42.30812619,-71.07692974]|[0.46555795830839225,0.47323330959508814]|\n",
      "|42.35945371|-71.05964817|[42.35945371,-71.05964817]|[0.7811704704741356,0.553613590996304]   |\n",
      "|42.37525782|-71.02466343|[42.37525782,-71.02466343]|[0.8783498171413008,0.7163351056474138]  |\n",
      "|42.29919694|-71.06046974|[42.29919694,-71.06046974]|[0.4106520710912284,0.5497922930592086]  |\n",
      "|42.32073413|-71.05676415|[42.32073413,-71.05676415]|[0.5430840810712674,0.5670277853760105]  |\n",
      "|42.33380683|-71.10377843|[42.33380683,-71.10377843]|[0.623468009376985,0.34835430511485943]  |\n",
      "|42.25614494|-71.12802506|[42.25614494,-71.12802506]|[0.14592566557305522,0.23557803869744187]|\n",
      "|42.348866  |-71.08936284|[42.348866,-71.08936284]  |[0.7160667259101776,0.4154043000380535]  |\n",
      "|42.34432328|-71.15778368|[42.34432328,-71.15778368]|[0.6881335767678302,0.09716432688119706] |\n",
      "|42.32324363|-71.10892316|[42.32324363,-71.10892316]|[0.5585149766080225,0.3244250645925437]  |\n",
      "|42.26059891|-71.1030614 |[42.26059891,-71.1030614] |[0.17331309167139522,0.3516893651339123] |\n",
      "|42.27986526|-71.08798275|[42.27986526,-71.08798275]|[0.29178172455616985,0.42182339405323893]|\n",
      "|42.27791927|-71.0964061 |[42.27791927,-71.0964061] |[0.2798158475266461,0.382644590082232]   |\n",
      "|42.31596119|-71.09042564|[42.31596119,-71.09042564]|[0.5137353109803878,0.4104609897327274]  |\n",
      "|42.28076737|-71.04736497|[42.28076737,-71.04736497]|[0.29732879176980825,0.610745383565361]  |\n",
      "|42.31277782|-71.07562922|[42.31277782,-71.07562922]|[0.49416079417431863,0.4792823065072909] |\n",
      "|42.34268073|-71.09937714|[42.34268073,-71.09937714]|[0.6780335498844389,0.3688256470547875]  |\n",
      "|42.30998781|-71.06213111|[42.30998781,-71.06213111]|[0.4770050448790654,0.5420649055526874]  |\n",
      "+-----------+------------+--------------------------+-----------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#faça aqui o exercício 01\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "df = dataset.select('Lat', 'Long').filter('Lat is not null').filter('Lat > 0').filter('Long is not null').filter('Long < -1')\n",
    "local_assembler = VectorAssembler(inputCols=[\"Lat\",\"Long\"], outputCol=\"Local\")\n",
    "dfv = local_assembler.transform(df)\n",
    "mms = MinMaxScaler(inputCol='Local', outputCol='LocalStd', max=1, min=0)\n",
    "mms.fit(dfv).transform(dfv).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f4536a1-b217-4f29-b960-e005e608fd63",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Imputer\n",
    "\n",
    "**Exemplo 3**: Tratando valores ausentes com Imputer, substituindo pela média"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92bd9a2-8899-4976-a025-3cff5730a3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----------------+---------------+\n",
      "| Lat|Long|           NeWLat|        NeWLong|\n",
      "+----+----+-----------------+---------------+\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "|NULL|NULL|42.21299505769182|-70.90603030527|\n",
      "+----+----+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "df = dataset.select(\"Lat\",\"Long\")\n",
    "imput = Imputer(inputCols=[\"Lat\",\"Long\"], outputCols = [\"NeWLat\",\"NeWLong\"])\n",
    "modelo = imput.fit(df)\n",
    "df = modelo.transform(df)\n",
    "df.filter('Lat is null').show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03084c66-d611-4891-a468-6c81a19c4cc5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### RFormula\n",
    "\n",
    "**Exemplo 4**: Utilizando o estimator RFormula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9be9c15-b653-4d6f-be5c-e0f2284c20e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+----+--------+------------------+-----+\n",
      "|  OFFENSE_CODE_GROUP|YEAR|MONTH|HOUR|DISTRICT|          features|label|\n",
      "+--------------------+----+-----+----+--------+------------------+-----+\n",
      "|  Disorderly Conduct|2018|   10|  20|     E18|[2018.0,10.0,20.0]| 26.0|\n",
      "|       Property Lost|2018|    8|  20|     D14| [2018.0,8.0,20.0]| 12.0|\n",
      "|               Other|2018|   10|  19|      B2|[2018.0,10.0,19.0]|  4.0|\n",
      "|  Aggravated Assault|2018|   10|  20|      A1|[2018.0,10.0,20.0]| 14.0|\n",
      "|            Aircraft|2018|   10|  20|      A7|[2018.0,10.0,20.0]| 57.0|\n",
      "|           Vandalism|2018|   10|  20|     C11|[2018.0,10.0,20.0]|  7.0|\n",
      "|     Verbal Disputes|2018|   10|  19|      B2|[2018.0,10.0,19.0]|  8.0|\n",
      "|      Simple Assault|2018|   10|  19|     E18|[2018.0,10.0,19.0]|  6.0|\n",
      "|               Towed|2018|   10|  20|      D4|[2018.0,10.0,20.0]|  9.0|\n",
      "|Motor Vehicle Acc...|2018|   10|  19|     D14|[2018.0,10.0,19.0]|  0.0|\n",
      "|          Auto Theft|2018|   10|  20|     E13|[2018.0,10.0,20.0]| 19.0|\n",
      "|  Medical Assistance|2018|   10|  17|     E18|[2018.0,10.0,17.0]|  2.0|\n",
      "|  Investigate Person|2018|   10|   8|      B3| [2018.0,10.0,8.0]|  3.0|\n",
      "|  Medical Assistance|2018|   10|  19|      B3|[2018.0,10.0,19.0]|  2.0|\n",
      "|               Other|2018|   10|  18|     C11|[2018.0,10.0,18.0]|  4.0|\n",
      "|               Other|2018|   10|  19|      B2|[2018.0,10.0,19.0]|  4.0|\n",
      "|  Investigate Person|2018|   10|  18|      D4|[2018.0,10.0,18.0]|  3.0|\n",
      "|Fire Related Reports|2018|   10|  18|     C11|[2018.0,10.0,18.0]| 27.0|\n",
      "|Motor Vehicle Acc...|2018|   10|  20|      C6|[2018.0,10.0,20.0]|  0.0|\n",
      "|             Larceny|2018|   10|  19|      D4|[2018.0,10.0,19.0]|  1.0|\n",
      "+--------------------+----+-----+----+--------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "df = dataset\\\n",
    "  .select('OFFENSE_CODE_GROUP', 'YEAR', 'MONTH', 'HOUR', 'DISTRICT')\\\n",
    "  .where('DISTRICT is not null')\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ . -DISTRICT')\n",
    "rf.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb818b4d-ef8c-4c57-9bd5-d7978a40b80d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Exemplo 4**: Estruturando atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af37dea-487e-4d0e-b7b3-e2bac06585cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----+--------------------+-----+\n",
      "|  OFFENSE_CODE_GROUP|DAY_OF_WEEK|HOUR|            features|label|\n",
      "+--------------------+-----------+----+--------------------+-----+\n",
      "|  Disorderly Conduct|  Wednesday|  20|(7,[1,6],[1.0,20.0])| 26.0|\n",
      "|       Property Lost|   Thursday|  20|(7,[2,6],[1.0,20.0])| 12.0|\n",
      "|               Other|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  4.0|\n",
      "|  Aggravated Assault|  Wednesday|  20|(7,[1,6],[1.0,20.0])| 14.0|\n",
      "|            Aircraft|  Wednesday|  20|(7,[1,6],[1.0,20.0])| 57.0|\n",
      "|           Vandalism|    Tuesday|  20|(7,[3,6],[1.0,20.0])|  7.0|\n",
      "|Motor Vehicle Acc...|  Wednesday|  20|(7,[1,6],[1.0,20.0])|  0.0|\n",
      "|     Verbal Disputes|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  8.0|\n",
      "|      Simple Assault|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  6.0|\n",
      "|               Towed|  Wednesday|  20|(7,[1,6],[1.0,20.0])|  9.0|\n",
      "|Motor Vehicle Acc...|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  0.0|\n",
      "|          Auto Theft|     Monday|  20|(7,[4,6],[1.0,20.0])| 19.0|\n",
      "|  Medical Assistance|  Wednesday|  17|(7,[1,6],[1.0,17.0])|  2.0|\n",
      "|  Investigate Person|  Wednesday|   8| (7,[1,6],[1.0,8.0])|  3.0|\n",
      "|  Medical Assistance|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  2.0|\n",
      "|Motor Vehicle Acc...|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  0.0|\n",
      "|               Other|  Wednesday|  18|(7,[1,6],[1.0,18.0])|  4.0|\n",
      "|               Other|  Wednesday|  19|(7,[1,6],[1.0,19.0])|  4.0|\n",
      "|  Investigate Person|  Wednesday|  18|(7,[1,6],[1.0,18.0])|  3.0|\n",
      "|Fire Related Reports|  Wednesday|  18|(7,[1,6],[1.0,18.0])| 27.0|\n",
      "+--------------------+-----------+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = dataset\\\n",
    "  .select('OFFENSE_CODE_GROUP', 'DAY_OF_WEEK', 'HOUR')\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ DAY_OF_WEEK + HOUR')\n",
    "rf.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e78b5067-1150-4f00-baaa-82f2e721aa9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 2\n",
    "\n",
    "Utilizando RFormula, prepare um DataFrame para uma tarefa de classificação, onde o label será `OFFENSE_CODE_GROUP`, e os atributos serão: `DISTRICT`, `DAY_OF_WEEK` e `HOUR`.\n",
    "\n",
    "Será necessário tratar valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce0559c-3fa5-417b-be58-b011279ec5ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-----------+----+--------------------+-----+\n",
      "|  OFFENSE_CODE_GROUP|DISTRICT|DAY_OF_WEEK|HOUR|            features|label|\n",
      "+--------------------+--------+-----------+----+--------------------+-----+\n",
      "|  Disorderly Conduct|     E18|  Wednesday|  20|(18,[8,12,17],[1....| 26.0|\n",
      "|       Property Lost|     D14|   Thursday|  20|(18,[6,13,17],[1....| 12.0|\n",
      "|               Other|      B2|  Wednesday|  19|(18,[0,12,17],[1....|  4.0|\n",
      "|  Aggravated Assault|      A1|  Wednesday|  20|(18,[3,12,17],[1....| 14.0|\n",
      "|            Aircraft|      A7|  Wednesday|  20|(18,[9,12,17],[1....| 57.0|\n",
      "|           Vandalism|     C11|    Tuesday|  20|(18,[1,14,17],[1....|  7.0|\n",
      "|     Verbal Disputes|      B2|  Wednesday|  19|(18,[0,12,17],[1....|  8.0|\n",
      "|      Simple Assault|     E18|  Wednesday|  19|(18,[8,12,17],[1....|  6.0|\n",
      "|               Towed|      D4|  Wednesday|  20|(18,[2,12,17],[1....|  9.0|\n",
      "|Motor Vehicle Acc...|     D14|  Wednesday|  19|(18,[6,12,17],[1....|  0.0|\n",
      "|          Auto Theft|     E13|     Monday|  20|(18,[7,15,17],[1....| 19.0|\n",
      "|  Medical Assistance|     E18|  Wednesday|  17|(18,[8,12,17],[1....|  2.0|\n",
      "|  Investigate Person|      B3|  Wednesday|   8|(18,[4,12,17],[1....|  3.0|\n",
      "|  Medical Assistance|      B3|  Wednesday|  19|(18,[4,12,17],[1....|  2.0|\n",
      "|               Other|     C11|  Wednesday|  18|(18,[1,12,17],[1....|  4.0|\n",
      "|               Other|      B2|  Wednesday|  19|(18,[0,12,17],[1....|  4.0|\n",
      "|  Investigate Person|      D4|  Wednesday|  18|(18,[2,12,17],[1....|  3.0|\n",
      "|Fire Related Reports|     C11|  Wednesday|  18|(18,[1,12,17],[1....| 27.0|\n",
      "|Motor Vehicle Acc...|      C6|    Tuesday|  20|(18,[5,14,17],[1....|  0.0|\n",
      "|             Larceny|      D4|  Wednesday|  19|(18,[2,12,17],[1....|  1.0|\n",
      "+--------------------+--------+-----------+----+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#faça aqui o exercício 02\n",
    "\n",
    "df = dataset\\\n",
    "    .select('OFFENSE_CODE_GROUP', 'DISTRICT', 'DAY_OF_WEEK', 'HOUR')\\\n",
    "    .where('DISTRICT is not null')\n",
    "\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ .')\n",
    "rf.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ace75c-13e0-4694-ad7a-61ea69a16bd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Classificação\n",
    "\n",
    "Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb423c82-a5cd-43ce-8eab-1ad3368d27d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(54,[0,1,2,3,5,6,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "|(54,[0,2,3,4,5,6,...|  5.0|\n",
      "|(54,[0,1,2,3,4,5,...|  5.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/19 01:58:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "dataset_location = 'covtype.data'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location) \\\n",
    "               .withColumnRenamed('_c54', 'class')\n",
    "\n",
    "rf = RFormula(formula = 'class ~ .')\n",
    "bInput = rf.fit(dataset) \\\n",
    "           .transform(dataset) \\\n",
    "           .select('features', 'label')\n",
    "\n",
    "bInput.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5bc3872-7c36-4808-b327-7e5d4629297a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d4e315-df15-4a24-aab7-83331dadb117",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/19 01:58:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/08/19 01:58:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e5c937-c3ad-4ad0-a429-f37e0955d96a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad90ea9a-667a-4f1a-afd8-9a238ce56b7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:===>                                                   (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7243241103453973\n",
      "precision by label:  [0.7117775658990879, 0.7479221545633475, 0.6741283124128312, 0.6035367940673132, 0.2007042253521127, 0.491006988200252, 0.7245859245120374]\n",
      "recall by label:  [0.6977530211480363, 0.8007702055411029, 0.811126027857023, 0.38514743356388786, 0.006004424312651428, 0.2467898888696954, 0.5737688932228181]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"accuracy: \", model.summary.accuracy)\n",
    "print(\"precision by label: \", model.summary.precisionByLabel)\n",
    "print(\"recall by label: \", model.summary.recallByLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ee549fb-8b96-45d1-8dc1-b3fe1e446a73",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0090680686743...|[1.81999741313856...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  3.0|[-9.0101641039548...|[1.55158307801299...|       3.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0085683805230...|[4.48359267947652...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0081713512667...|[2.51740347208869...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0087747208431...|[1.71659936908560...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0085582628801...|[3.12977099388803...|       1.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0087378161775...|[2.58193308253375...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0085232861942...|[1.84470268749794...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0080473656732...|[1.91954954671642...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0084714402608...|[1.52472370945344...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0087822929274...|[3.55251248038819...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0086487027275...|[2.62658699972610...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0083773708660...|[1.48625344922309...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0080832576509...|[1.20946279055865...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0089007829059...|[2.43685081599240...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0086628316521...|[2.20942545018665...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  1.0|[-9.0088093737801...|[6.28492600959812...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0084831090582...|[1.46144790403396...|       2.0|\n",
      "|(54,[0,1,2,3,4,5,...|  2.0|[-9.0085881973002...|[2.49511819458078...|       2.0|\n",
      "|(54,[0,1,2,5,6,7,...|  2.0|[-9.0087958540642...|[2.71440397575470...|       2.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test = bInput.sample(fraction=0.0001)\n",
    "model.transform(test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94598e3f-7c8d-4884-839c-1d221aeff578",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32369a6c-4149-4c0b-82d5-193a8828f0b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 289:==================================>                    (10 + 6) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6976323199816102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "train, test = bInput.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "model = dt.fit(train)\n",
    "predictions = model.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f6afa2-8250-440f-b695-71a8784b3602",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_a32b50314ca7, depth=5, numNodes=47, numClasses=8, numFeatures=54\n",
      "  If (feature 0 <= 3034.5)\n",
      "   If (feature 0 <= 2565.5)\n",
      "    If (feature 10 <= 0.5)\n",
      "     If (feature 0 <= 2456.5)\n",
      "      If (feature 3 <= 15.0)\n",
      "       Predict: 4.0\n",
      "      Else (feature 3 > 15.0)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2456.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "    Else (feature 10 > 0.5)\n",
      "     If (feature 22 <= 0.5)\n",
      "      Predict: 2.0\n",
      "     Else (feature 22 > 0.5)\n",
      "      If (feature 5 <= 1010.0)\n",
      "       Predict: 2.0\n",
      "      Else (feature 5 > 1010.0)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 2565.5)\n",
      "    If (feature 15 <= 0.5)\n",
      "     If (feature 0 <= 2939.5)\n",
      "      If (feature 17 <= 0.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 17 > 0.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 0 > 2939.5)\n",
      "      Predict: 2.0\n",
      "    Else (feature 15 > 0.5)\n",
      "     If (feature 9 <= 1365.5)\n",
      "      If (feature 7 <= 214.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 7 > 214.5)\n",
      "       Predict: 3.0\n",
      "     Else (feature 9 > 1365.5)\n",
      "      If (feature 5 <= 2281.5)\n",
      "       Predict: 3.0\n",
      "      Else (feature 5 > 2281.5)\n",
      "       Predict: 2.0\n",
      "  Else (feature 0 > 3034.5)\n",
      "   If (feature 0 <= 3310.5)\n",
      "    If (feature 7 <= 238.5)\n",
      "     If (feature 0 <= 3100.5)\n",
      "      If (feature 3 <= 166.0)\n",
      "       Predict: 1.0\n",
      "      Else (feature 3 > 166.0)\n",
      "       Predict: 2.0\n",
      "     Else (feature 0 > 3100.5)\n",
      "      Predict: 1.0\n",
      "    Else (feature 7 > 238.5)\n",
      "     If (feature 3 <= 316.0)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 316.0)\n",
      "      If (feature 0 <= 3205.5)\n",
      "       Predict: 2.0\n",
      "      Else (feature 0 > 3205.5)\n",
      "       Predict: 1.0\n",
      "   Else (feature 0 > 3310.5)\n",
      "    If (feature 12 <= 0.5)\n",
      "     Predict: 1.0\n",
      "    Else (feature 12 > 0.5)\n",
      "     If (feature 45 <= 0.5)\n",
      "      Predict: 7.0\n",
      "     Else (feature 45 > 0.5)\n",
      "      If (feature 5 <= 907.5)\n",
      "       Predict: 7.0\n",
      "      Else (feature 5 > 907.5)\n",
      "       Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1501955-0dad-4c44-beb1-cddcea2b1445",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 3\n",
    "\n",
    "* Carregue os dados do dataset `Crimes in Boston`\n",
    "* Pré-processe os dados para uma tarefa de classificação. A coluna `OFFENSE_CODE_GROUP` será usada como classe e as colunas `DISTRICT`, `DAY_OF_WEEK`, `HOUR` e `SHOOTING` serão usadas como atributos.\n",
    " * Registros sem informações de `DISTRICT` não devem entrar na análise\n",
    " * Registros sem informação de `SHOOTING` devem assumir valor `N` (dica: usar `expr` e `coalesce`)\n",
    "* Crie um modelo de Regressão Logística\n",
    "* Avalie o modelo pela métrica accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acec865-afc5-421c-84c4-754f408e8d1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(19,[8,12,17,18],...| 26.0|\n",
      "|(19,[6,13,17,18],...| 12.0|\n",
      "|(19,[0,12,17,18],...|  4.0|\n",
      "|(19,[3,12,17,18],...| 14.0|\n",
      "|(19,[9,12,17,18],...| 57.0|\n",
      "|(19,[1,14,17,18],...|  7.0|\n",
      "|(19,[0,12,17,18],...|  8.0|\n",
      "|(19,[8,12,17,18],...|  6.0|\n",
      "|(19,[2,12,17,18],...|  9.0|\n",
      "|(19,[6,12,17,18],...|  0.0|\n",
      "|(19,[7,15,17,18],...| 19.0|\n",
      "|(19,[8,12,17,18],...|  2.0|\n",
      "|(19,[4,12,17,18],...|  3.0|\n",
      "|(19,[4,12,17,18],...|  2.0|\n",
      "|(19,[1,12,17,18],...|  4.0|\n",
      "|(19,[0,12,17,18],...|  4.0|\n",
      "|(19,[2,12,17,18],...|  3.0|\n",
      "|(19,[1,12,17,18],...| 27.0|\n",
      "|(19,[5,14,17,18],...|  0.0|\n",
      "|(19,[2,12,17,18],...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.1333768854701484\n"
     ]
    }
   ],
   "source": [
    "#faça aqui o exercício 03\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "dataset_location = 'crime.csv'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', True) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location)\n",
    "\n",
    "\n",
    "df = dataset \\\n",
    "    .select('OFFENSE_CODE_GROUP', 'DISTRICT', 'DAY_OF_WEEK', 'HOUR', expr(\"coalesce(SHOOTING, 'N') as SHOOTING\"))\\\n",
    "    .where('DISTRICT is not null')\n",
    "\n",
    "\n",
    "rf = RFormula(formula = 'OFFENSE_CODE_GROUP ~ .')\n",
    "\n",
    "bInput = rf.fit(df) \\\n",
    "           .transform(df) \\\n",
    "           .select('features', 'label')\n",
    "\n",
    "bInput.show()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(bInput)\n",
    "print(\"accuracy: \", model.summary.accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57c756ab-f822-4f64-9057-2cda87047d90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Agrupamento\n",
    "\n",
    "Preparação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b21c47a6-74a1-4c5a-8a25-f65cbcb841f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|class|\n",
      "+--------------------+-----+\n",
      "|(54,[0,1,2,3,5,6,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    2|\n",
      "|(54,[0,1,2,3,4,5,...|    2|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    2|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    2|\n",
      "|(54,[0,1,2,3,4,5,...|    2|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "|(54,[0,2,3,4,5,6,...|    5|\n",
      "|(54,[0,1,2,3,4,5,...|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_location = 'covtype.data'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location) \\\n",
    "               .withColumnRenamed('_c54', 'class')\n",
    "\n",
    "rf = RFormula(formula = ' ~ . -class')\n",
    "bInput = rf.fit(dataset) \\\n",
    "           .transform(dataset) \\\n",
    "           .select('features', 'class')\n",
    "\n",
    "bInput.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfa8edb0-3a45-4ecc-813b-f4c770e93e8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1228c5e4-8421-42b0-90a0-c2cbc8759744",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.75299212e+03, 1.62645112e+02, 1.68385153e+01, 2.42078791e+02,\n",
       "        5.14256915e+01, 8.64830428e+02, 2.07967543e+02, 2.17810744e+02,\n",
       "        1.39436955e+02, 9.92690362e+02, 2.12917776e-01, 8.72231793e-02,\n",
       "        4.68266249e-01, 2.31592796e-01, 1.89882537e-02, 3.65105717e-02,\n",
       "        2.99577134e-02, 4.42349256e-02, 1.00046985e-02, 4.11902897e-02,\n",
       "        0.00000000e+00, 0.00000000e+00, 7.18559123e-03, 1.50490211e-01,\n",
       "        3.82458888e-02, 2.20516836e-02, 3.53390760e-02, 3.75254503e-03,\n",
       "        1.87940486e-05, 5.20595145e-03, 9.82928739e-03, 1.06499608e-04,\n",
       "        2.14878622e-03, 8.54502741e-03, 2.16131558e-03, 5.11699295e-02,\n",
       "        9.03617854e-02, 3.83837118e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.25293657e-03, 5.02427565e-03, 7.75567737e-02, 4.57509789e-02,\n",
       "        4.07329679e-02, 8.11965544e-02, 7.51699295e-02, 1.39702428e-03,\n",
       "        8.01879405e-04, 0.00000000e+00, 0.00000000e+00, 7.86217698e-03,\n",
       "        8.85199687e-03, 8.51996868e-03]),\n",
       " array([3.03076806e+03, 1.51583231e+02, 1.13186983e+01, 3.35043511e+02,\n",
       "        4.50416743e+01, 4.99858234e+03, 2.15530324e+02, 2.27778271e+02,\n",
       "        1.45391523e+02, 4.41941577e+03, 9.26308883e-01, 0.00000000e+00,\n",
       "        7.36911167e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.75554389e-03, 3.09670647e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.44757906e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.80553733e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.11009054e-02, 4.41411888e-02, 0.00000000e+00, 3.17543629e-02,\n",
       "        7.67615798e-02, 3.72654507e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.71801601e-01, 8.34011285e-02,\n",
       "        0.00000000e+00, 4.98622228e-04, 1.08647159e-02, 0.00000000e+00,\n",
       "        3.41162577e-03, 0.00000000e+00, 0.00000000e+00, 3.07571185e-02,\n",
       "        8.58155098e-03, 2.09946201e-02]),\n",
       " array([3.01175437e+03, 1.39748754e+02, 1.35126149e+01, 2.84719837e+02,\n",
       "        4.58301616e+01, 1.40985531e+03, 2.15800029e+02, 2.22118180e+02,\n",
       "        1.37747294e+02, 2.39583646e+03, 3.85780591e-01, 9.46465133e-02,\n",
       "        5.19572896e-01, 0.00000000e+00, 0.00000000e+00, 6.27278882e-03,\n",
       "        0.00000000e+00, 1.85328689e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.16162424e-02,\n",
       "        3.10376612e-02, 4.69602669e-02, 2.69183395e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.91691206e-03, 1.19174831e-02, 5.58759472e-03,\n",
       "        5.23684060e-03, 1.51068984e-02, 1.03594822e-03, 5.05983213e-02,\n",
       "        1.07583630e-01, 4.61445596e-02, 3.72778217e-03, 1.03023827e-02,\n",
       "        0.00000000e+00, 1.17461845e-03, 1.69185843e-01, 3.99696557e-02,\n",
       "        5.26212753e-02, 1.29183559e-01, 1.20993858e-01, 1.44380185e-03,\n",
       "        5.00028550e-03, 0.00000000e+00, 0.00000000e+00, 3.15270856e-02,\n",
       "        2.58742343e-02, 1.25292635e-02]),\n",
       " array([2.89560590e+03, 1.20231971e+02, 1.12659820e+01, 1.91843417e+02,\n",
       "        2.50338617e+01, 2.56915651e+03, 2.19885800e+02, 2.23728399e+02,\n",
       "        1.36104599e+02, 5.37955567e+03, 8.69837017e-01, 1.30162983e-01,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.22795267e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.32343529e-01, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 2.61591129e-02, 0.00000000e+00, 4.45411922e-02,\n",
       "        5.35833891e-03, 6.62350227e-02, 0.00000000e+00, 6.06534197e-02,\n",
       "        5.87928853e-02, 1.04189923e-02, 6.32581677e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 3.63660043e-01, 7.15561509e-02,\n",
       "        0.00000000e+00, 1.53308030e-02, 8.93056486e-03, 0.00000000e+00,\n",
       "        1.74890228e-03, 0.00000000e+00, 0.00000000e+00, 4.42807174e-03,\n",
       "        1.90518717e-02, 8.93056486e-03]),\n",
       " array([3.06473031e+03, 1.51090265e+02, 1.34903567e+01, 2.78528093e+02,\n",
       "        4.51007731e+01, 3.15618940e+03, 2.14200765e+02, 2.24057162e+02,\n",
       "        1.41591451e+02, 2.53172647e+03, 5.86189572e-01, 5.59618176e-03,\n",
       "        4.08214246e-01, 0.00000000e+00, 0.00000000e+00, 2.37244613e-03,\n",
       "        0.00000000e+00, 6.25209333e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.77933460e-02,\n",
       "        1.88679245e-02, 8.42916155e-02, 9.76889584e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.57697890e-03, 1.89795691e-03, 0.00000000e+00,\n",
       "        2.51200179e-03, 1.22111198e-02, 0.00000000e+00, 4.02478508e-02,\n",
       "        1.02545495e-01, 5.80272413e-02, 0.00000000e+00, 1.24204533e-02,\n",
       "        4.88444792e-03, 0.00000000e+00, 2.36211901e-01, 6.56888467e-02,\n",
       "        5.65479513e-02, 7.25549849e-02, 6.94568494e-02, 0.00000000e+00,\n",
       "        1.24204533e-03, 0.00000000e+00, 3.23769119e-03, 3.51540694e-02,\n",
       "        5.69805739e-02, 2.72552194e-02]),\n",
       " array([3.11771186e+03, 1.66088134e+02, 1.15965983e+01, 2.64675715e+02,\n",
       "        3.79836320e+01, 5.02212180e+03, 2.12069091e+02, 2.28261471e+02,\n",
       "        1.49621166e+02, 1.54320325e+03, 9.11529653e-01, 0.00000000e+00,\n",
       "        8.84703471e-02, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 4.48437675e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.31914348e-03, 3.02695431e-03, 0.00000000e+00,\n",
       "        0.00000000e+00, 3.50742325e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        2.90203238e-02, 1.21878954e-02, 0.00000000e+00, 1.44060603e-01,\n",
       "        1.08537933e-01, 7.12695591e-03, 0.00000000e+00, 1.23320361e-03,\n",
       "        0.00000000e+00, 0.00000000e+00, 4.44946268e-01, 1.09755121e-01,\n",
       "        1.34531303e-03, 1.56953186e-02, 2.06441487e-02, 1.40937555e-03,\n",
       "        3.20312625e-05, 0.00000000e+00, 0.00000000e+00, 6.06511956e-02,\n",
       "        2.22296962e-02, 6.82265892e-03]),\n",
       " array([3.04034623e+03, 1.71880606e+02, 1.42921681e+01, 2.86680826e+02,\n",
       "        5.16461592e+01, 2.56042691e+03, 2.09535319e+02, 2.28171420e+02,\n",
       "        1.50223235e+02, 1.07962807e+03, 2.19845568e-01, 4.60288809e-03,\n",
       "        7.75551544e-01, 0.00000000e+00, 0.00000000e+00, 7.60128359e-03,\n",
       "        4.11151223e-04, 2.62234256e-02, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 4.70016045e-02,\n",
       "        1.15122343e-02, 2.44384276e-02, 7.62234256e-02, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.61572403e-04, 2.56718813e-03, 0.00000000e+00,\n",
       "        4.78339350e-03, 9.47653430e-03, 3.67027677e-03, 4.29502607e-02,\n",
       "        1.15413157e-01, 3.19995989e-02, 0.00000000e+00, 3.60008022e-03,\n",
       "        5.37505014e-03, 0.00000000e+00, 9.69414360e-02, 1.30966707e-02,\n",
       "        8.60108303e-02, 1.71590453e-01, 1.14340152e-01, 1.12615323e-02,\n",
       "        8.84476534e-03, 1.19334136e-03, 6.61853189e-04, 2.86401925e-02,\n",
       "        2.91917369e-02, 2.44183714e-02])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(7)\n",
    "model = kmeans.fit(bInput)\n",
    "\n",
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5faff66d-34b5-492b-b487-511b31b6383a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|class|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(54,[0,1,2,3,5,6,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    2|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    2|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    2|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    2|         1|\n",
      "|(54,[0,1,2,3,4,5,...|    2|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "|(54,[0,2,3,4,5,6,...|    5|         3|\n",
      "|(54,[0,1,2,3,4,5,...|    5|         3|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(bInput)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64be45c7-50fb-4c61-b92b-eb3f900dfe94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 599:=====================================>                 (11 + 5) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "silhouette:  0.5070710973839342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"silhouette: \", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052fe035-135e-455b-9c92-9c31ca744d50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Exercício 4\n",
    "\n",
    "* Carregue os dados do dataset `Crimes in Boston`\n",
    "* Pré-processe os dados para uma tarefa de agrupamento. As colunas `Lat` (latitude) e `Long` (longitude) serão usadas como atributos.\n",
    " * Remover ou imputar valores nulos ou inconsistentes para ambos os atributos\n",
    "* Crie um modelo de agrupamento pelo algoritmo k-means\n",
    "* Avalie o modelo pelo índice silhueta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "149825f8-fb8f-46b1-8957-1c9363c041a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------------------+\n",
      "|        Lat|        Long|            features|\n",
      "+-----------+------------+--------------------+\n",
      "|42.26260773|-71.12118637|[42.26260773,-71....|\n",
      "|42.35211146|-71.13531147|[42.35211146,-71....|\n",
      "|42.30812619|-71.07692974|[42.30812619,-71....|\n",
      "|42.35945371|-71.05964817|[42.35945371,-71....|\n",
      "|42.37525782|-71.02466343|[42.37525782,-71....|\n",
      "|42.29919694|-71.06046974|[42.29919694,-71....|\n",
      "|42.32073413|-71.05676415|[42.32073413,-71....|\n",
      "|42.33380683|-71.10377843|[42.33380683,-71....|\n",
      "|42.25614494|-71.12802506|[42.25614494,-71....|\n",
      "|  42.348866|-71.08936284|[42.348866,-71.08...|\n",
      "|42.34432328|-71.15778368|[42.34432328,-71....|\n",
      "|42.32324363|-71.10892316|[42.32324363,-71....|\n",
      "|42.26059891| -71.1030614|[42.26059891,-71....|\n",
      "|42.27986526|-71.08798275|[42.27986526,-71....|\n",
      "|42.27791927| -71.0964061|[42.27791927,-71....|\n",
      "|42.31596119|-71.09042564|[42.31596119,-71....|\n",
      "|42.28076737|-71.04736497|[42.28076737,-71....|\n",
      "|42.31277782|-71.07562922|[42.31277782,-71....|\n",
      "|42.34268073|-71.09937714|[42.34268073,-71....|\n",
      "|42.30998781|-71.06213111|[42.30998781,-71....|\n",
      "+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "silhouette:  0.6068551831366734\n"
     ]
    }
   ],
   "source": [
    "#faça aqui o exercício 04\n",
    "\n",
    "dataset_location = 'crime.csv'\n",
    "\n",
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', True) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location)\n",
    "\n",
    "df = dataset \\\n",
    "    .select('Lat', 'Long') \\\n",
    "    .where('Lat is not null and Lat > 0') \\\n",
    "    .where('Long is not null and Long < -1')\n",
    "\n",
    "rf = RFormula(formula = ' ~ .')\n",
    "\n",
    "bInput = rf.fit(df).transform(df)\n",
    "bInput.show()\n",
    "\n",
    "kmeans = KMeans().setK(5)\n",
    "model = kmeans.fit(bInput)\n",
    "predictions = model.transform(bInput)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"silhouette: \", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1b48f7-5d11-48e6-94c1-bda15c8e5187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pipeline\n",
    "\n",
    "É possível executar todos os objetos (modelos e transformações) instanciados em um *pipeline* de aprendizado de máquina. Isso facilita a aplicação de modelos e reutilização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e284ff-3ec3-42b1-839d-dcfc839b13bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:30:50 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.046875\n",
      "24/08/19 02:30:51 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:30:51 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "24/08/19 02:30:51 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------+---------------------------------------------------------+---------------------------------------------------+----------+\n",
      "|target|features         |rawprediction                                            |probability                                        |prediction|\n",
      "+------+-----------------+---------------------------------------------------------+---------------------------------------------------+----------+\n",
      "|0.0   |[4.3,3.0,1.1,0.1]|[176.44407418010744,75.68930974099888,-252.0275446345717]|[1.0,1.7488871172240973E-44,8.263048400568215E-187]|0.0       |\n",
      "|0.0   |[4.4,3.2,1.3,0.2]|[176.44407418010744,75.68930974099888,-252.0275446345717]|[1.0,1.7488871172240973E-44,8.263048400568215E-187]|0.0       |\n",
      "|0.0   |[4.6,3.1,1.5,0.2]|[176.44407418010744,75.68930974099888,-252.0275446345717]|[1.0,1.7488871172240973E-44,8.263048400568215E-187]|0.0       |\n",
      "|0.0   |[4.8,3.0,1.4,0.1]|[176.44407418010744,75.68930974099888,-252.0275446345717]|[1.0,1.7488871172240973E-44,8.263048400568215E-187]|0.0       |\n",
      "|0.0   |[4.8,3.1,1.6,0.2]|[176.44407418010744,75.68930974099888,-252.0275446345717]|[1.0,1.7488871172240973E-44,8.263048400568215E-187]|0.0       |\n",
      "+------+-----------------+---------------------------------------------------------+---------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "0.9545454545454546\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "iris = spark.read.csv(\"iris.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "irisTreino, irisTeste = iris.randomSplit([0.7,0.3])\n",
    "\n",
    "vector = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],outputCol=\"features\" )\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"target\")\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[4,5,3], featuresCol=\"features\", labelCol=\"target\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector, indexer, mlp])\n",
    "modelo = pipeline.fit(irisTreino)\n",
    "previsao = modelo.transform(irisTeste)\n",
    "\n",
    "previsao.select(\"target\",\"features\",\"rawprediction\",\"probability\",\"prediction\").show(5, truncate=False)\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\",predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = performance.evaluate(previsao)\n",
    "print(acuracia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04bb6e75-b369-42b2-8650-a001795fd522",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Crossvalidation e Tunning\n",
    "\n",
    "Ao desenvolver aplicações com aprendizado de máquina, é possível buscar por parametrização adequada aos dados. É possível buscar pela melhor configuração utilizando *grid search* em conjunto com um bom estimador como validação cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597d9edd-ebf6-4616-8a75-eec21c483349",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/19 02:31:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:31:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:31:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:31:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.015625\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0078125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0087890625\n",
      "24/08/19 02:31:34 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.015625\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0078125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.00390625\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.001953125\n",
      "24/08/19 02:31:34 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 9.765625E-4\n",
      "24/08/19 02:31:34 ERROR LBFGS: Failure again! Giving up and returning. Maybe the objective is just poorly behaved?\n",
      "24/08/19 02:31:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 3.796875\n",
      "24/08/19 02:31:41 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:31:49 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 3.796875\n",
      "24/08/19 02:31:49 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:14 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.09375\n",
      "24/08/19 02:32:14 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:32:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:21 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:22 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.09375\n",
      "24/08/19 02:32:22 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.015625\n",
      "24/08/19 02:32:33 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.017578125\n",
      "24/08/19 02:32:34 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.03125\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.015625\n",
      "24/08/19 02:32:38 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.017578125\n",
      "24/08/19 02:32:38 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n",
      "24/08/19 02:32:40 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.5\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.25\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.125\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.0625\n",
      "24/08/19 02:32:41 ERROR StrongWolfeLineSearch: Encountered bad values in function evaluation. Decreasing step size to 0.046875\n",
      "24/08/19 02:32:41 ERROR LBFGS: Failure! Resetting history: breeze.optimize.FirstOrderException: Line search zoom failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------+-----------------------------------------------------------------+----------+\n",
      "|rawprediction                                            |probability                                                      |prediction|\n",
      "+---------------------------------------------------------+-----------------------------------------------------------------+----------+\n",
      "|[26.836414490217727,44.19286633291703,-70.93182310341365]|[2.8986029289230287E-8,0.9999999710139709,1.0045756207150348E-50]|1.0       |\n",
      "|[26.836414490217727,44.19286633291703,-70.93182310341365]|[2.8986029289230287E-8,0.9999999710139709,1.0045756207150348E-50]|1.0       |\n",
      "|[26.836414490217727,44.19286633291703,-70.93182310341365]|[2.8986029289230287E-8,0.9999999710139709,1.0045756207150348E-50]|1.0       |\n",
      "|[26.836414490217727,44.19286633291703,-70.93182310341365]|[2.8986029289230287E-8,0.9999999710139709,1.0045756207150348E-50]|1.0       |\n",
      "|[26.836414490217727,44.19286633291703,-70.93182310341365]|[2.8986029289230287E-8,0.9999999710139709,1.0045756207150348E-50]|1.0       |\n",
      "+---------------------------------------------------------+-----------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "iris = spark.read.csv(\"iris.csv\", header=True, inferSchema=True, sep=\",\")\n",
    "irisTreino, irisTeste = iris.randomSplit([0.8,0.2])\n",
    "\n",
    "vector = VectorAssembler(inputCols=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"],outputCol=\"features\" )\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"species\", outputCol=\"target\")\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=100, layers=[4,5,3], featuresCol=\"features\", labelCol=\"target\")\n",
    "\n",
    "pipeline = Pipeline(stages=[vector, indexer, mlp])\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\", metricName=\"accuracy\")\n",
    "\n",
    "grid = ParamGridBuilder().addGrid(mlp.maxIter,[10,100,1000]).addGrid(mlp.layers,[[4,4,4,3],[4,6,3]]).build()\n",
    "\n",
    "#A validação cruzada tem seu próprio avaliador e vai subdividir os dados de treino para o processo de tunning\n",
    "crossval = CrossValidator(estimator=pipeline,estimatorParamMaps=grid,evaluator=performance,numFolds=5)\n",
    "\n",
    "#Podemos utilizar e avaliar novamente o modelos escolhido pela validação cruzada (validação da validação) usando hold out\n",
    "modelo = crossval.fit(irisTreino)\n",
    "previsao = modelo.transform(irisTeste)\n",
    "previsao.select(\"rawprediction\",\"probability\",\"prediction\").show(5, truncate=False)\n",
    "\n",
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = performance.evaluate(previsao)\n",
    "print(acuracia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "EML4 Aula 05 - Aluno",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "name": "Aula 7",
  "notebookId": 4064479383097879
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
